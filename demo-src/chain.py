import re
import glob
import codecs
import requests
import argparse
import functools
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_core.vectorstores import VST
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from typing import List, Dict, Any


class LLMClient:
    def __init__(self, base_url: str='http://llm:8888'):
        self.base_url = base_url

    def generate(self, prompt: str):
        '''
        Returns response from LLM server for a given prompt.
        '''
        response = requests.post(f'{self.base_url}/generate', json={'prompt': prompt})
        if response.status_code == 200:
            return response.json()['response'][0]['generated_text']
        else:
            raise Exception(f"Error: {response.status_code}, {response.text}")


def build_excerpts_vector_db(data_path: str='./data'):
    '''
    Loads the dataset excerpts and stores them in vector DB.

    Args:
        data_path: path to directory with excerpts

    Returns:
        Vector DB of dataset excerpts
    '''
    loader = DirectoryLoader(data_path, glob="**/*.txt", loader_cls=TextLoader)
    documents = loader.load()

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    excerpts_vector_db = FAISS.from_documents(documents, embeddings)

    return excerpts_vector_db


def retrieve_matching_datasets(
    excerpts_vector_db: VST, 
    user_query: str, 
    data_path: str,
    top_k: int=2, 
    verbose: bool=False
):
    '''
    Retrieves top k matching datasets given the query.

    Args: 
        excerpts_vector_db: vector DB of dataset excerpts
        user_query: user query
        data_path: path to datasets directory
        top_k: number of top matching documents to return
        verbose: whether to output progress

    Returns:
        Top k matching datasets
    '''        
    # Search the database for similar excerpts
    matching_excerpts = excerpts_vector_db.similarity_search(user_query, k=top_k)
    excerpt_sources = map(lambda doc: doc.metadata['source'], matching_excerpts)

    if verbose:
        print(f"User query: {user_query}")
        print(f"Top {top_k} relevant documents:")

    matching_datasets = []
    for i, excerpt_path in enumerate(excerpt_sources):

        if verbose:
            print(f"{i+1}. Excerpt path: {excerpt_path} \n")

        with open(excerpt_path) as f:
            excerpt = f.read()

        dataset_id = excerpt_path.split('/')[-1].split('.')[0]
            
        resources_dir = f'{data_path}/datasets/{dataset_id}/resources'
        resources_paths = glob.glob(resources_dir + '/*.csv')

        matching_datasets.append({
            'id' : dataset_id,
            'excerpt': excerpt,
            'resources_paths': resources_paths,
        })
    
    return matching_datasets


def load_dataframes(datasets: List[Dict]) -> Dict[str, Dict[str, pd.DataFrame]]:
    '''
    Loads dataset resources as dataframes.

    Args:
        datasets: list of datasets

    Returns:
        Dictionary of loaded pandas dataframes
    '''
    dfs = {}
    for dataset in datasets:
        dfs[dataset['id']] = {}
        for path in dataset['resources_paths']:
            df_name = path.split('/')[-1].split('.')[0]
            # Replace invalid characters with underscores and ensure it doesn't start with a number
            df_name = re.sub(r'\W|^(?=\d)', '_', df_name)
    
            with codecs.open(path, 'r', encoding='utf-8', errors='ignore') as f:
                dfs[dataset['id']][df_name] = pd.read_csv(f)
    
    return dfs


def generate_information_retrieval_code(
    llm_client: LLMClient,
    user_query: str,
    matching_datasets: List[Dict],
    matching_dfs: Dict[str, Dict[str, pd.DataFrame]],
    df_head_n: int=3,
    verbose: bool=False
) -> str:
    '''
    Generates information retrieval code by providing the LLM with descriptions of relevant dataset dataframes.

    Args:
        llm_client: LLM client
        user_query: user query
        matching_datasets: list of matching datasets
        matching_dfs: dictionary of matching dataframes
        df_head_n: number of rows to include in datafame head
        verbose: whether to output progress
    
    Returns:
        Information retrieval code generated by the LLM.
    '''
    MAX_USER_QUERY_LEN = 1000
    MAX_EXCERPT_LEN = 2000
    MAX_DF_HEAD_LEN = 2000

    user_query = user_query[:MAX_USER_QUERY_LEN]

    data_info = ''
    for dataset in matching_datasets:
        
        excerpt = dataset['excerpt'][:MAX_EXCERPT_LEN]

        
        data_info += f'Dataset id: {dataset["id"]}\n'
        data_info += f'Dataset description (short): {excerpt}\n\n'
        
        for df_name in matching_dfs[dataset['id']]:

            df = matching_dfs[dataset['id']][df_name]
            df_head = df.head(n=df_head_n).to_csv(index=False)[:MAX_DF_HEAD_LEN]

            data_info += f'DataFrame name (parent dataset {dataset["id"]}): {df_name}\n'
            data_info += f'DataFrame head (column names and first few data rows):\n{df_head}\n\n'

        data_info += '\n'
    
    prompt = f'''
You are a specialized python code generation model.
You will be presented with short descriptions of the datasets relevant to the users query as well as pandas dataframe heads associated with each of the datasets and the user query itself. 
Each dataset may contain more than one pandas dataframe.
Your task is to generate complete python code to retrieve the information the user is asking for from the available pandas dataframes following the given template:
```python
# your code here
information_retrieval_result = # your answer
```
You must:
* Make sure that you are referencing the dataframe variables in code with their respective names as specified, instead of just df
* Make sure to import the libraries which you are going to use. The allowed libraries are: pandas, numpy and any built-in python library
* Make sure to save the final answer in the variable called information_retrieval_result

The following is the user query: {user_query}

The following is the information on datasets and the associated dataframes: 
{data_info}
    '''

    if verbose: print(f'Information retrieval code generation prompt: \n {prompt}')

    response = llm_client.generate(prompt).strip()

    code_start = response.find("```")
    code_end = response.rfind("```")
    
    if code_start != -1 and code_end != -1:
        info_retrieval_code = response[code_start+10: code_end].strip()
    else:
        info_retrieval_code = '\n'.join(
            [line for line in response.split('\n') if '```' not in line]
        ).strip()

    if verbose: print(f'Generated information retrieval code: \n {info_retrieval_code}')

    return info_retrieval_code


def execute_information_retrieval_code(
    information_retrieval_code: str,
    matching_dfs: Dict[str, Dict[str, pd.DataFrame]], 
    verbose: bool=False
) -> Any:
    '''
    Executes the information retrieval code.

    Args:
        information_retrieval_code: information retrieval code
        matching_dfs: dictionary of matching dataframes
        verbose: whether to output progress

    Returns:
        information retrieval result
    '''
    MAX_RETRIES = 5

    globals().update(functools.reduce(
        lambda acc, dfs: acc | dfs, matching_dfs.values(), {}
    ))
    
    attempt = 0
    while attempt < MAX_RETRIES:
        try:
            exec(information_retrieval_code, globals())
            result = globals().get('information_retrieval_result', None)

            if verbose:
                print(f'Information retrieval result: {result}')
                print(f'Attempted {attempt} retries')
            
            return result
        except Exception as e:
            if verbose: print(f"Information retrieval unsuccessful: {e}")
            attempt += 1
    
    return "Information retrieval unsuccessful"


def generate_final_response(
    llm_client: LLMClient,
    user_query: str,
    matching_dfs: Dict[str, Dict[str, pd.DataFrame]],
    information_retrieval_code: str,
    information_retrieval_result: Any,
    verbose: bool=False
):
    '''
    Generates the final response by providing the LLM with information retrieval result and sources, if applicable.

    Args:
        llm_client: LLM client
        user_query: user query
        matching_dfs: dictionary of matching dataframes
        information_retrieval_code: information retrieval code
        information_retrieval_result: information retrieval result
        verbose: whether to output progress
    '''
    sources = []
    for dataset_id in matching_dfs:
        for df_name in matching_dfs[dataset_id]:
            if df_name in information_retrieval_code:
                sources.append((dataset_id, df_name))

    prompt = f'''
You are the generator model in a RAG system.
You will be provided with the user query, the result of information retrieval from the knowledge base and the sources used for retrieval.
Your task is to generate the final response to user's query given the information retrieved.
If the result of information retrieval indicates that the retrieval was unsucessful, you must explicitly mention it.
If applicable, you must cite the sources used for the purpose of information retrieval.
The sources will be listed as tuples (<dataset id>, <dataframe name>) where dataset id is the parent dataset and dataframe name is the name of specific dataframe within the dataset.

The following is the user query: {user_query}

The following is the information retrieval result: {information_retrieval_result}

The following is the list of sources: {sources}
    '''
    response = llm_client.generate(prompt).strip()

    if verbose: print(f'Final response: {response}')
    
    return response


def run_chain(
    llm_client: LLMClient,
    excerpts_vector_db: VST,
    user_query: str, 
    data_path: str='./data',
    top_k_datasets: int=2,
    df_head_n: int=3,
    verbose: bool=False
) -> str:
    '''
    Runs the whole RAG chain and returns the final response.

    Args:
        llm_client: LLM client,
        excerpts_vector_db: vector DB of dataset excerpts
        user_query: user query,
        top_k_datasets: number of top matching documents to return
        df_head_n: number of rows to include in datafame head
        verbose: whether to output progress

    Returns:
        Final response to the user's query
    '''
    matching_datasets = retrieve_matching_datasets(
        excerpts_vector_db, user_query, data_path, top_k_datasets, verbose
    )

    matching_dfs = load_dataframes(matching_datasets)

    information_retrieval_code = generate_information_retrieval_code(
        llm_client, user_query, matching_datasets, matching_dfs, df_head_n, verbose
    )
    information_retrieval_result = execute_information_retrieval_code(
        information_retrieval_code, matching_dfs, verbose
    )
    final_response = generate_final_response(
        llm_client, user_query, matching_dfs, information_retrieval_code, information_retrieval_result, verbose
    )

    return final_response


if __name__ == '__main__':
    # Example queries:
    # "List all indoor bike station addresses"
    # "How many ambulance stations are located in Toronto?"

    parser = argparse.ArgumentParser()
    parser.add_argument('--user_query', type=str, required=True)
    parser.add_argument('--data_path', type=str, default='./data')
    parser.add_argument('--llm_url', type=str, default='http://llm:8888')
    parser.add_argument('--top_k_datasets', type=int, default=2)
    parser.add_argument('--df_head_n', type=int, default=3)
    parser.add_argument('--verbose', type=bool, default=False)
    args = parser.parse_args()

    llm_client = LLMClient(args.llm_url)

    excerpts_vector_db = build_excerpts_vector_db(args.data_path)

    final_response = run_chain(
        llm_client, 
        excerpts_vector_db, 
        args.user_query, 
        args.data_path,
        args.top_k_datasets, 
        args.df_head_n,
        args.verbose
    )

    print(final_response)