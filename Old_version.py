import os
import glob
import numpy as np
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
import pandas as pd
import openai
from openai import OpenAI
import re

os.environ['OPENAI_API_KEY'] = # redacted key
openai.api_key = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
# Step 1: Load documents from the excerpts directory
loader = DirectoryLoader('data/excerpts', glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()

# Step 2: Create embeddings and store them in a Chroma vector store
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(documents, embeddings)

def process_query_only_sources(query, k=2):
    def map_to_resources(source):
        basename = os.path.basename(source)
        name = os.path.splitext(basename)[0]
        return "data/datasets/" + name + "/resources"
        
    # Search the database for similar documents
    relevant_docs = db.similarity_search(query, k=k)
    sources = map(lambda doc: doc.metadata['source'], relevant_docs)

    print(f"Query: {query}")
    print(f"Top {k} relevant documents:")
    results = []
    all_resources = []
    for i, source in enumerate(sources, 1):
        print(f"{i}. Source: {source}")
        print()
        with open(source) as f:
            excerpt = f.read()
        resources_dir = map_to_resources(source)
        resources = glob.glob(resources_dir + '/*.csv')
        all_resources.extend(resources)

        results.append({
            'resources_dir': resources_dir,
            'excerpt': excerpt,
            'resources': resources,
        })
    

    return results, all_resources

import pandas as pd
import openai
from openai import OpenAI
import os
import re

# Initialize OpenAI with your API key

def sanitize_dataframe_name(name):
    # Replace invalid characters with underscores and ensure it doesn't start with a number
    sanitized_name = re.sub(r'\W|^(?=\d)', '_', name)
    return sanitized_name

def load_csv_files(file_paths):
    dataframes = {}
    for file_path in file_paths:
        df_name = sanitize_dataframe_name(file_path.split('/')[-1].split('.')[0])
        dataframes[df_name] = pd.read_csv(file_path)
    return dataframes

def generate_context(dataframes):
    context = ""
    for name, df in dataframes.items():
        context += f"exact data frame name for df name is:{name}.\nSample Column names and rows:\n{df.head().to_csv(index=False)}\n\n"
    return context

def generate_pandas_query(question, context):
    prompt = f"""
    Given the following DataFrame information:
    {context}

    Answer the following question with a complete python pandas code from import to print use real data from DataFrame Information. 
    All data are are already existing dataframes.
    Ensure the query references the correct DataFrame by name:
    {question}

    Makesure to use the corresponding exact original name from the data frame name instead of just df. Do not generate any comments.
    """
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=150
    )
    response_text = response.choices[0].message.content.strip()
    # Extract the actual query from the response
    query_start = response_text.find("```")
    query_end = response_text.rfind("```")
    
    if query_start != -1 and query_end != -1:
        query = response_text[query_start+10:query_end].strip()
    else:
        lines = response_text.split('\n')
        query_lines = [line for line in lines if 'import pandas as pd' not in line and '```' not in line and 'You can find' not in line]
        query = '\n'.join(query_lines).strip()
    return query

def execute_pandas_query(dataframes, query):
    local_vars = locals()
    local_vars.update(dataframes)
    
    max_retries = 5
    attempt = 0
    
    while attempt < max_retries:
        try:
            # Use exec to execute multi-line code
            exec(query, globals(), local_vars)
            result = local_vars.get('result', None)
            print(f'Attempted {attempt} retries')
            globals()['result'] = result
            return result
        except Exception as e:
            attempt += 1
            if attempt == max_retries:
                print("retrieval unsuccessful, result generated by LLM alone.")
                return None

def generate_natural_language_answer(question, result, selected_dataframe_name):
    prompt = f"""
    Given the result of the query and the name of the selected DataFrame:
    Query Result: {result}
    DataFrame Name: {selected_dataframe_name}

    Answer the following question in natural language:
    {question}
    """
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=150
    )
    answer = response.choices[0].message.content.strip()
    return answer

def main(file_paths, question):
    # Step 1: Retrieve relevant CSVs
    #relevant_csvs = retrieve_relevant_csvs(question, file_paths)
    relevant_csvs = file_paths
    # Step 2: Load CSV files
    dataframes = load_csv_files(relevant_csvs)
    
    # Step 3: Generate context from CSV files
    context = generate_context(dataframes)
    print(context)
    
    # Step 4: Generate pandas query using OpenAI's GPT model
    pandas_query = generate_pandas_query(question, context)
    print(f"Generated Pandas Query:\n{pandas_query}\n")
    
    # Step 5: Execute the pandas query
    finalresult = execute_pandas_query(dataframes, pandas_query)
    # Extract and return the result
    print(finalresult)

    selected_dataframe_name = None
    for name in dataframes.keys():
        if name in pandas_query:
            selected_dataframe_name = name
            break
    
    answer = generate_natural_language_answer(question, finalresult, selected_dataframe_name)
    
    return answer, selected_dataframe_name


# Example usage
query = "How many ambulance stations located in Toronto"
sources, all_resources = process_query_only_sources(query)
print(all_resources)
result = main(all_resources, query)
print(f'Data is from:{result[1]}')

#query = "How many ambulance stations located in Toronto"
#query = "What are the indoor bike station addresses"
#query = "Where are the traffic cameras"

#query that will not work
#query = "How many rats are there in Toronto"
